# -*- coding: utf-8 -*-
"""Copy of AMES_housing_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a4F8ROhAg3Z2JIkdGg8hHLiPaouLit07

# Замечание: прежде чем мы сможем выполнить проверочный проект по линейной регрессии, нам нужно выполнить построение (конструирование) признаков - feature engineering.

Link for Google docs Bekzod: https://docs.google.com/spreadsheets/d/1yzKD-GlsVajYMOQMXiUAqwsb75rSfITt997wjSHLg88/edit?gid=0#gid=0

---
----
# Feature engineering.

Мы будем работать со следующим набором данных:

* Ames Iowa Data Set: http://jse.amstat.org/v19n3/decock.pdf
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import matplotlib.pyplot as plt
import seaborn as sns

"""Open DF
---
"""

# df = pd.read_csv("..\DATA\Ames_Housing_Data.csv")

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("") # file path with uploaded file

df_traditional = df.copy()

df.head()

print(df.shape, df_traditional.shape)

threshold = 0.8
columns_to_drop = []
for col in df_traditional.columns:
  if df_traditional[col].isnull().sum() / len(df_traditional) > threshold:
    columns_to_drop.append(col)
    print(f"Dropping {col} due to high % of missing values (> {threshold*100:.0f}%)")

"""Check dummy correlactions
---

Encoding Categories to Numbers OneHotEncoding
---
"""

#identifying
string_cols = df_traditional.select_dtypes(include=['object']).columns

encoder = OneHotEncoder(sparse_output = False, handle_unknown='ignore')

encoded_data = encoder.fit_transform(df_traditional[string_cols])

encoded_df = pd.DataFrame(encoded_data, columns = encoder.get_feature_names_out(string_cols))

df_traditional = pd.concat([df_traditional, encoded_df], axis=1)

df_traditional = df_traditional.drop(columns = string_cols)

"""Check Correlations
---
"""

print(df.shape, df_traditional.shape)

correlation = df.corr(numeric_only=True)['SalePrice'].sort_values(ascending=False)

print("Корреляция числовых признаков с SalePrice:")
print(correlation)

plt.figure(figsize=(10, 6))
correlation.head(15).plot(kind='bar')
plt.title('Топ-15 числовых признаков по корреляции с SalePrice')
plt.ylabel('Коэффициент корреляции')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

for feature in correlation.head(15).index:
  plt.figure(figsize=(8,6))
  sns.scatterplot(x=df_traditional[feature], y=df_traditional['SalePrice'])
  plt.title(f'Dependency of SalePrice on {feature}')
  plt.xlabel(feature)
  plt.ylabel('SalePrice')
  plt.tight_layout()
  plt.show()

"""Working with NaNs
---
"""

#identify columns with higher % of missing values
threshold = 0.8
columns_to_drop = []
for col in df_traditional.columns:
  if df_traditional[col].isnull().sum() / len(df_traditional) > threshold:
    columns_to_drop.append(col)
    print(f"Dropping {col} due to high % of missing values (> {threshold*100:.0f}%)")

# drop identified columns
df_traditional = df_traditional.drop(columns = columns_to_drop)

for col in df_traditional.select_dtypes(include=['number']).columns:
  if df_traditional[col].isnull().any():
    df_traditional[col] = df_traditional[col].fillna(df_traditional[col].median())

nan_counts = []
for col in df_traditional.columns:
  nan_count = df_traditional[col].isna().sum()
  nan_counts.append((col, nan_count))
nan_counts

"""Data Splitting
---
"""

#Features
X = df_traditional.drop('SalePrice', axis=1)

#Target
y = df_traditional['SalePrice']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)

"""Model Training
---
"""

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
mse_test = mean_squared_error(y_test, y_pred)
rmse_test = np.sqrt(mse_test)
r2 = r2_score(y_test, y_pred)

print(f"Mean absolute error: {mae}")
print(f"Mean squared error: {mse_test}")
print(f"Root mean squared error: {rmse_test}")
print(f"R-squared: {r2}")

y_test.min()

y_test.max()

y_test.mean()

y_test.std()

y_pred.min()

y_pred.max()

y_pred.mean()

y_pred.std()

"""REGULARIZATION
---
"""

X_reg = df_traditional.drop('SalePrice', axis=1)
y_reg = df_traditional['SalePrice']

X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X,y,test_size=0.2, random_state = 42)

param_grid = {
    'alpha' : [0.001, 0.01, 0.1, 1.0, 10, 100, 1000]
}

# Rigde regression
ridge_model = Ridge()
ridge_grid = GridSearchCV(ridge_model, param_grid, scoring='neg_mean_squared_error', cv=5 )
ridge_grid.fit(X_reg_train, y_reg_train)

best_ridge_alpha = ridge_grid.best_params_['alpha']
best_ridge_model = ridge_grid.best_estimator_

# Approximating model to test data and outpur metrics
ridge_pred = best_ridge_model.predict(X_reg_test)
ridge_mse = mean_squared_error(y_reg_test, ridge_pred)
ridge_rmse = np.sqrt(ridge_mse)
ridge_r2_score = r2_score(y_reg_test, ridge_pred)
ridge_mae = mean_absolute_error(y_reg_test, ridge_pred)

# print(f"Ridgge Regression MSE: {ridge_mse}")
print(f"Ridgge Regression RMSE: {ridge_rmse}")
print(f"Ridgge Regression R2_Score: {ridge_r2_score}")
print(f"Ridgge Regression MAE: {ridge_mae}")

# Lasso Regression
lasso_model = Lasso()
lasso_grid = GridSearchCV(lasso_model, param_grid, scoring='neg_mean_squared_error', cv=5 )
lasso_grid.fit(X_reg_train, y_reg_train)

best_lasso_alpha = lasso_grid.best_params_['alpha']
best_lasso_model = lasso_grid.best_estimator_

#Approxmating model values to test data
lasso_pred = best_lasso_model.predict(X_reg_test)
lasso_mse = mean_squared_error(y_reg_test, lasso_pred)
lasso_rmse = np.sqrt(lasso_mse)
lasso_r2_score = r2_score(y_reg_test, lasso_pred)
lasso_mae = mean_absolute_error(y_reg_test, lasso_pred)

# print(f"Lasso regression MSE: {lasso_mse}")
print(f"Lasso Regression RMSE: {lasso_rmse}")
print(f"Lasso Regression R2_Score: {lasso_r2_score}")
print(f"Lasso Regression MAE: {lasso_mae}")

"""----
----

Metrics calculating
---
"""

min_error = abs(y_pred - y_test).min()
min_error

max_error = abs(y_pred - y_test).max()
max_error

min_reg_error = abs(ridge_pred - y_test).min()
min_reg_error

max_reg_error = abs(ridge_pred - y_test).max()
max_reg_error

ridge_pred.min()

ridge_pred.max()

ridge_pred.mean()

ridge_pred.std()

lasso_pred.min()

lasso_pred.max()

lasso_pred.mean()

lasso_pred.std()

"""**Checking for overfitting of the LinearRegression**
---

"""

X_of_train, X_temp, y_of_train, y_temp = train_test_split(X, y, test_size = 0.3, random_state=42)
X_val, X_of_test, y_val, y_of_test = train_test_split(X_temp, y_temp, test_size = 0.5, random_state=42)
y_train_pred = model.predict(X_of_train)
y_val_pred = model.predict(X_val)

# Calculate RMSE for training and validation sets
train_rmse = np.sqrt(mean_squared_error(y_of_train, y_train_pred))
val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))

print(f"Training RMSE: {train_rmse}")
print(f"Validation RMSE: {val_rmse}")

train_sizes, train_scores, val_scores = learning_curve(
    model, X, y, cv=10, scoring='neg_mean_squared_error',
    train_sizes = np.linspace(0.1, 1.0, 10)
)

#Calculate RMSE for training and validation sets
train_rmse_scores = np.sqrt(-train_scores)
val_rmse_scores = np.sqrt(-val_scores)

#Calculate mean and standard deviation of RMSE scores
train_rmse_mean = np.mean(train_rmse_scores, axis=1)
train_rmse_std = np.std(train_rmse_scores, axis=1)
val_rmse_mean = -np.mean(val_scores, axis=1)
val_rmse_std = np.std(val_rmse_scores, axis=1)

# Plot RMSE learning curve
plt.plot(train_sizes, train_rmse_mean, label='Training RMSE')
plt.plot(train_sizes, val_rmse_mean, label='Validation RMSE')
plt.fill_between(train_sizes, train_rmse_mean - train_rmse_std, train_rmse_mean + train_rmse_std, alpha=0.1)
plt.fill_between(train_sizes, val_rmse_mean - val_rmse_std, val_rmse_mean + val_rmse_std, alpha = 0.1)
plt.xlabel('Training set size')
plt.ylabel('RMSE')
plt.title('LinearRegrassion overfitting')
plt.legend()
plt.show()

"""**Checking for overfitting of the Ridge**
---

"""

X_ofr_train, X_temp, y_ofr_train, y_temp = train_test_split(X, y, test_size = 0.3, random_state=42)
X_val, X_ofr_test, y_val, y_ofr_test = train_test_split(X_temp, y_temp, test_size = 0.5, random_state=42)
y_train_pred = ridge_grid.predict(X_ofr_train)
y_val_pred = ridge_grid.predict(X_val)

train_rmse = np.sqrt(mean_squared_error(y_ofr_train, y_train_pred))
val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))
print(f"Training RMSE: {train_rmse}")
print(f"Validation RMSE: {val_rmse}")

train_sizes, train_scores, val_scores = learning_curve(
    ridge_grid, X, y, cv=10, scoring='neg_mean_squared_error',
    train_sizes = np.linspace(0.1, 1.0, 10)
)

#Calculate RMSE for training and validation scores
train_rmse_scores = np.sqrt(-train_scores)
val_rmse_scores = np.sqrt(-val_scores)

# Calculate mean and standard deviation of RMSE scores
train_rmse_mean = np.mean(train_rmse_scores, axis=1)
train_rmse_std = np.std(train_rmse_scores, axis=1)
val_rmse_mean = np.mean(val_rmse_scores, axis=1)
val_rmse_std = np.std(val_rmse_scores, axis=1)

# Plot RMSE learning curve
plt.plot(train_sizes, train_rmse_mean, label='Training RMSE')
plt.plot(train_sizes, val_rmse_mean, label='Validation RMSE')
plt.fill_between(train_sizes, train_rmse_mean - train_rmse_std, train_rmse_mean + train_rmse_std, alpha=0.1)
plt.fill_between(train_sizes, val_rmse_mean - val_rmse_std, val_rmse_mean + val_rmse_std, alpha = 0.1)
plt.xlabel('Training set size')
plt.ylabel('RMSE')
plt.title('Ridge Regularization overfitting')
plt.legend()
plt.show()

"""**Checking for overfitting Lasso**
---
"""

X_ofl_train, X_temp, y_ofl_train, y_temp = train_test_split(X, y, test_size = 0.3, random_state=42)
X_val, X_ofl_test, y_val, y_ofl_test = train_test_split(X_temp, y_temp, test_size = 0.5, random_state=42)
y_train_pred = lasso_grid.predict(X_ofl_train)
y_val_pred = lasso_grid.predict(X_val)

# Calcualte
train_mse = mean_squared_error(y_ofl_train, y_train_pred)
val_mse = mean_squared_error(y_val, y_val_pred)
print(f"Training MSE: {train_mse}")
print(f"Validation MSE: {val_mse}")

train_sizes, train_scores, val_scores = learning_curve(
    lasso_grid, X, y, cv=5, scoring='neg_mean_squared_error',
    train_sizes = np.linspace(0.1, 1.0, 10)
)

train_mean = -np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
val_mean = -np.mean(val_scores, axis=1)
val_std = np.std(val_scores, axis=1)

plt.plot(train_sizes, train_mean, label='Training error')
plt.plot(train_sizes, val_mean, label='Validation error')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)
plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha = 0.1)
plt.xlabel('Training set size')
plt.ylabel('MSE')
plt.title('Ridge Regularization overfitting')
plt.legend()
plt.show()
